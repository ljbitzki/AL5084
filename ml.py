'''
Start of Machine Learning models module
'''
from __future__ import annotations
from pathlib import Path
from datetime import datetime
from typing import Union, Tuple, Dict, Any
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
import joblib

PathLike = Union[str, Path]

def _ensure_dir(path: PathLike) -> Path:
    """
    Ensures that the 'path' directory exists.
    If 'path' has a suffix (looks like a file), it creates the parent;
    if it doesn't, it creates the path itself as a directory.

    Args:
        p (Path): Path to try if exists
    """
    p = Path(path)
    if p.suffix:
        p.parent.mkdir(parents=True, exist_ok=True)
    else:
        p.mkdir(parents=True, exist_ok=True)
    return p

def load_dataset(dataset_path: PathLike) -> pd.DataFrame:
    """
    Loads the CSV dataset generated by build_dataset_unsupervised.

    Args:
        dataset_path (PathLike): Path to dataset

    Raises:
        FileNotFoundError: Dataset CSV not found

    Returns:
        pd.DataFrame: Pandas DataFrame
    """
    p = Path(dataset_path)
    if not p.is_file():
        raise FileNotFoundError(f"Dataset CSV not found: {p}")
    df = pd.read_csv(p, engine="python")
    return df

def prepare_features_for_anomaly_detection(
    df: pd.DataFrame,
) -> Tuple[pd.DataFrame, pd.DataFrame, list]:
    """
    Separate:
    - X: only numeric columns (features)
    - meta: columns useful for flow identification (IPs, ports, etc.)
    - feature_names: list of column names in X
    Also:
    - drop numeric columns with all NaN values
    - replace NaN with median (per column), fallback=0
    - drop columns with zero variance (only 1 distinct value)

    Args:
        df (pd.DataFrame): Pandas DataFrame

    Raises:
        ValueError: No numeric columns found in dataset for anomaly detection
        ValueError: All numeric columns are empty (NaN). Nothing to train on
        ValueError: No non-constant numeric features left after cleaning

    Returns:
        Tuple[pd.DataFrame, pd.DataFrame, list]: Pandas DataFrame Tuple
    """

    df = df.copy()

    # Meta columns that we want to load along with the scores (when they exist)
    meta_cols_candidates = [
        "flow_id",
        "src_ip", "dst_ip",
        "src_port", "dst_port",
        "proto",
        "timestamp", "stime", "ltime",
        "source_tool", "__source_csv",
        "ntflow_label",     # ntflowlyzer label (heuristic)
        "handshake_state",  # typical ntflowlyzer field
    ]
    meta_cols = [c for c in meta_cols_candidates if c in df.columns]
    meta = df[meta_cols].copy() if meta_cols else pd.DataFrame(index=df.index)

    # Select only numeric columns for features.
    numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
    if not numeric_cols:
        raise ValueError("No numeric columns found in dataset for anomaly detection.")

    X = df[numeric_cols].copy()

    # Remove columns with all NaN tags.
    X = X.dropna(axis=1, how="all")

    if X.shape[1] == 0:
        raise ValueError("All numeric columns are empty (NaN). Nothing to train on.")

    # Replace NaN with the column median (fallback=0)
    for col in X.columns:
        col_data = X[col]
        if col_data.isna().all():
            X[col] = 0.0
        else:
            median = col_data.median()
            X[col] = col_data.fillna(median)

    # Remove columns with zero variance (only 1 distinct value).
    nunique = X.nunique(dropna=False)
    constant_cols = nunique[nunique <= 1].index.tolist()
    if constant_cols:
        X = X.drop(columns=constant_cols)

    feature_names = X.columns.tolist()
    if not feature_names:
        raise ValueError("No non-constant numeric features left after cleaning.")

    return X, meta, feature_names

def train_isolation_forest(
    X: pd.DataFrame,
    *,
    contamination: float = 0.05,
    random_state: int = 42,
    n_estimators: int = 200,
    n_jobs: int = -1,
) -> IsolationForest:
    """
    Train an IsolationForest on the X features.
    - contamination: approximate fraction of expected anomalies (e.g., 0.05 = 5%)
    - random_state: initial random state. Any integer
    - n_estimators: number of individual isolation trees that are built
    - n_jobs: CPU cores utilized for the computations

    Args:
        X (pd.DataFrame): X Pandas DataFrame
        contamination (float, optional): IsolationForest contamination. Defaults to 0.05.
        random_state (int, optional): IsolationForest initial random state. Defaults to 42.
        n_estimators (int, optional): IsolationForest estimators. Defaults to 200.
        n_jobs (int, optional): IsolationForest jobs. Defaults to -1.

    Returns:
        IsolationForest: IsolationForest model results
    """

    model = IsolationForest(
        n_estimators=n_estimators,
        contamination=contamination,
        random_state=random_state,
        n_jobs=n_jobs,
        verbose=0,
    )
    model.fit(X)
    return model

def score_anomalies(
    model: IsolationForest,
    X: pd.DataFrame,
    meta: pd.DataFrame,
) -> pd.DataFrame:
    """
    Calculates anomaly scores and binary label (-1 anomalous, 1 normal) and
    returns a DataFrame containing:
    - meta columns (IPs, ports, flow_id, etc.)
    - score columns: anomaly_score, is_anomaly, rank

    Args:
        model (IsolationForest): model
        X (pd.DataFrame): X Pandas DataFrame
        meta (pd.DataFrame): Meta Pandas DataFrame

    Returns:
        pd.DataFrame: Pandas DataFrame
    """

    # score_samples: The more negative, the more anomalous.
    scores = model.score_samples(X)
    # Inverting the sign means that LARGER values ​​= more anomalous.
    anomaly_score = -scores

    # prediction: -1 = anomalous, 1 = normal
    preds = model.predict(X)

    result = meta.copy() if not meta.empty else pd.DataFrame(index=X.index)
    result["anomaly_score"] = anomaly_score
    result["is_anomaly"] = (preds == -1).astype(int)

    # rank: 1 = most suspicious (highest anomaly_score)
    result["rank"] = result["anomaly_score"].rank(
        method="first", ascending=False
    ).astype(int)

    # Optional: Sort by rank
    result = result.sort_values("rank")

    return result


def run_anomaly_detection(
    dataset_path: PathLike,
    *,
    models_dir: PathLike = "models",
    scores_dir: PathLike = "datasets",
    contamination: float = 0.05,
    random_state: int = 42,
    n_estimators: int = 200,
    save_model: bool = True,
    save_scores: bool = True,
) -> Dict[str, Any]:
    """
    Complete Anomaly Detection Pipeline:
    1. Loads the single CSV dataset (build_dataset_unsupervised).
    2. Separates numerical features and metadata.
    3. Trains IsolationForest.
    4. Calculates anomaly scores.
    5. Saves:
    - model in models_dir
    - CSV with scores in scores_dir
    Returns a dictionary with basic information (paths, sizes, etc.).

    Args:
        dataset_path (PathLike): Dataset output path
        models_dir (PathLike, optional): Models output path. Defaults to "models".
        scores_dir (PathLike, optional): Dataset scores output path. Defaults to "datasets".
        contamination (float, optional): IsolationForest contamination. Defaults to 0.05.
        random_state (int, optional): IsolationForest initial random state. Defaults to 42.
        n_estimators (int, optional): IsolationForest estimators. Defaults to 200.
        save_model (bool, optional): Save joblib model. Defaults to True.
        save_scores (bool, optional): Save joblib scores. Defaults to True.

    Returns:
        Dict[str, Any]: Train results
    """

    # 1) Load dataset
    df = load_dataset(dataset_path)

    # 2) Prepare features
    X, meta, feature_names = prepare_features_for_anomaly_detection(df)

    # 3) Model training
    model = train_isolation_forest(
        X,
        contamination=contamination,
        random_state=random_state,
        n_estimators=n_estimators,
    )

    # 4) Anomaly score
    scored = score_anomalies(model, X, meta)

    # Output paths
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    dataset_name = Path(dataset_path).stem

    models_dir = _ensure_dir(models_dir)
    scores_dir = _ensure_dir(scores_dir)

    model_path = models_dir / f"iforest.{timestamp}.joblib"
    scores_path = scores_dir / f"{dataset_name}.iforest_scores.{timestamp}.csv"

    # 5) Save model and scores
    if save_model:
        joblib.dump(
            {
                "model": model,
                "feature_names": feature_names,
                "trained_on": str(dataset_path),
                "contamination": contamination,
                "timestamp": timestamp,
            },
            model_path,
        )

    if save_scores:
        scored.to_csv(scores_path, index=False)

    return {
        "model_path": str(model_path) if save_model else None,
        "scores_path": str(scores_path) if save_scores else None,
        "n_samples": int(X.shape[0]),
        "n_features": int(X.shape[1]),
        "contamination": contamination,
    }
